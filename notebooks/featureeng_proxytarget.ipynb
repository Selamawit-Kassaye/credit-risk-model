{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7967d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Desktop\\\\KAIM\\\\Week_4\\\\credit-risk-model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make project root visible so we can import src modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "project_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b4f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature_Engineering.py - Complete Task 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382e673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 00:21:35,339 - INFO - Loading data from C:\\Users\\User\\Desktop\\KAIM\\Week_4\\credit-risk-model\\Data\\data.csv...\n",
      "2025-12-30 00:21:38,602 - INFO - Feature DataFrame shape: (3742, 13)\n",
      "2025-12-30 00:21:38,608 - INFO - Target distribution: 0 (low risk)=3550, 1 (high risk)=192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (3742, 13)\n",
      "Target distribution:\n",
      " total_amount\n",
      "0    3550\n",
      "1     192\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load raw data\n",
    "# ============================================================\n",
    "X, y = load_data()\n",
    "print(\"Raw data shape:\", X.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84c743af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load data\n",
    "# ============================================================\n",
    "DATA_PATH = r\"C:\\Users\\User\\Desktop\\KAIM\\Week_4\\credit-risk-model\\Data\\data.csv\"\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['TransactionStartTime'])\n",
    "\n",
    "# ============================================================\n",
    "# Aggregate Features per Customer\n",
    "# ============================================================\n",
    "agg_features = (\n",
    "    df.groupby(\"CustomerId\")\n",
    "      .agg(\n",
    "          total_amount=pd.NamedAgg(column=\"Amount\", aggfunc=\"sum\"),\n",
    "          avg_amount=pd.NamedAgg(column=\"Amount\", aggfunc=\"mean\"),\n",
    "          transaction_count=pd.NamedAgg(column=\"Amount\", aggfunc=\"count\"),\n",
    "          std_amount=pd.NamedAgg(column=\"Amount\", aggfunc=\"std\"),\n",
    "          total_value=pd.NamedAgg(column=\"Value\", aggfunc=\"sum\"),\n",
    "          avg_value=pd.NamedAgg(column=\"Value\", aggfunc=\"mean\"),\n",
    "          std_value=pd.NamedAgg(column=\"Value\", aggfunc=\"std\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(agg_features, on=\"CustomerId\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41448881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Time-based Features\n",
    "# ============================================================\n",
    "df['transaction_hour'] = df['TransactionStartTime'].dt.hour\n",
    "df['transaction_day'] = df['TransactionStartTime'].dt.day\n",
    "df['transaction_month'] = df['TransactionStartTime'].dt.month\n",
    "df['transaction_year'] = df['TransactionStartTime'].dt.year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a14a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Handle Missing Values\n",
    "# ============================================================\n",
    "numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with mode\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d583b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV values per categorical feature:\n",
      "{'TransactionId': np.float64(4.066381783677512), 'BatchId': np.float64(4.146530053337502), 'AccountId': np.float64(6.845183723260969), 'SubscriptionId': np.float64(6.844113209456131), 'CustomerId': np.float64(6.65362707926726), 'CurrencyCode': np.float64(0.0), 'ProviderId': np.float64(3.3227451830919335), 'ProductId': np.float64(3.868466779726453), 'ProductCategory': np.float64(1.063636562098841), 'ChannelId': np.float64(1.2231102812260919)}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WoE and IV\n",
    "# ============================================================\n",
    "\n",
    "def calc_woe_iv(df, feature, target):\n",
    "    eps = 0.0001\n",
    "    grouped = df.groupby(feature)[target].agg(['count','sum'])\n",
    "    grouped['good'] = grouped['count'] - grouped['sum']\n",
    "    grouped['bad'] = grouped['sum']\n",
    "    grouped['dist_good'] = grouped['good'] / grouped['good'].sum()\n",
    "    grouped['dist_bad'] = grouped['bad'] / grouped['bad'].sum()\n",
    "    grouped['woe'] = np.log((grouped['dist_good'] + eps) / (grouped['dist_bad'] + eps))\n",
    "    grouped['iv'] = (grouped['dist_good'] - grouped['dist_bad']) * grouped['woe']\n",
    "    iv = grouped['iv'].sum()\n",
    "    woe_dict = grouped['woe'].to_dict()\n",
    "    return woe_dict, iv\n",
    "\n",
    "target_col = 'FraudResult'  # \n",
    "\n",
    "woe_cols = []\n",
    "iv_dict = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    woe_dict, iv = calc_woe_iv(df, col, target_col)\n",
    "    df[col + \"_woe\"] = df[col].map(woe_dict)\n",
    "    woe_cols.append(col + \"_woe\")\n",
    "    iv_dict[col] = iv\n",
    "\n",
    "print(\"IV values per categorical feature:\")\n",
    "print(iv_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c6ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-Hot Encoding for remaining non-WoE categorical columns\n",
    "remaining_cat_cols = list(set(categorical_cols) - set([c.replace(\"_woe\",\"\") for c in woe_cols]))\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # updated parameter\n",
    "ohe_cols = ohe.fit_transform(df[remaining_cat_cols])\n",
    "ohe_df = pd.DataFrame(ohe_cols, columns=ohe.get_feature_names_out(remaining_cat_cols))\n",
    "df = pd.concat([df.reset_index(drop=True), ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0cd4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Normalize/Standardize numeric features\n",
    "# ============================================================\n",
    "scaler = StandardScaler()\n",
    "scaled_numeric = scaler.fit_transform(df[numeric_cols])\n",
    "scaled_numeric_df = pd.DataFrame(scaled_numeric, columns=numeric_cols)\n",
    "df.update(scaled_numeric_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04751f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete. Dataset ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Final clean dataset ready for modeling\n",
    "# ============================================================\n",
    "# Select columns for modeling\n",
    "model_cols = numeric_cols + woe_cols + list(ohe_df.columns)\n",
    "df_model = df[model_cols + [target_col]]\n",
    "\n",
    "\n",
    "df_model.to_csv(r\"C:\\Users\\User\\Desktop\\KAIM\\Week_4\\credit-risk-model\\Data\\cleaned_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering complete. Dataset ready for modeling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cac72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Task 4 - Proxy Target Variable Engineering\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fb5bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Calculate RFM Metrics\n",
    "# ============================================================\n",
    "\n",
    "# Set a snapshot date for recency calculation\n",
    "snapshot_date = df['TransactionStartTime'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Ensure TransactionStartTime is datetime\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "\n",
    "# Aggregate RFM per customer\n",
    "rfm = df.groupby('CustomerId').agg(\n",
    "    recency=('TransactionStartTime', lambda x: (snapshot_date - x.max()).days),\n",
    "    frequency=('TransactionId', 'count'),\n",
    "    monetary=('Amount', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f17fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\User\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Scale RFM Features\n",
    "# ============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "# ============================================================\n",
    "# 3. K-Means Clustering\n",
    "# ============================================================\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "rfm['cluster'] = kmeans.fit_predict(rfm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc75031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy target variable 'is_high_risk' created and merged successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Identify High-Risk Cluster\n",
    "# ============================================================\n",
    "\n",
    "# Compute mean RFM values per cluster\n",
    "cluster_summary = rfm.groupby('cluster')[['recency', 'frequency', 'monetary']].mean()\n",
    "# High-risk cluster typically has:\n",
    "# - high recency (long time since last transaction)\n",
    "# - low frequency\n",
    "# - low monetary\n",
    "high_risk_cluster = cluster_summary.sort_values(\n",
    "    by=['recency', 'frequency', 'monetary'],\n",
    "    ascending=[False, True, True]\n",
    ").index[0]\n",
    "\n",
    "# Assign binary target\n",
    "rfm['is_high_risk'] = (rfm['cluster'] == high_risk_cluster).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Merge Target Variable Back Into Main DataFrame\n",
    "# ============================================================\n",
    "\n",
    "# Keep only CustomerId and is_high_risk\n",
    "target_df = rfm[['CustomerId', 'is_high_risk']]\n",
    "\n",
    "# Merge into main processed dataset\n",
    "df = df.merge(target_df, on='CustomerId', how='left')\n",
    "\n",
    "# ============================================================\n",
    "# 6. Optional: Save processed dataset\n",
    "# ============================================================\n",
    "\n",
    "df.to_csv(\"processed_data_with_target.csv\", index=False)\n",
    "\n",
    "print(\"Proxy target variable 'is_high_risk' created and merged successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3d9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
